Randall Pittman
Summer Scholars Research Plan
-----------------------------

1. Benchmarking  (5 weeks)
==========================
a. Week 1
---------
i. Complete the setup of the algorithm timers (Python: time.time() or
timeit?)

ii. Figure out how to deal with high CPU benchmark variance.

iii. Determine whether to include any form of IO writing
(i.e. to hard drive) when running the algorithm.
Or we can have some hard drive access initially to load
data into host memory, and simply start the timer after
this is done. 

iv. Determine how to run a benchmark (i.e. running a python program
that makes a shell call to run a program that prints to stdout its
execution time and append this to the end of a benchmark file(?)) 

v. Begin writing Mandelbrot set algorithm. (Maybe only the C++
sequential this week).

b. Week 2
---------
i. Finish Mandelbrot CUDA algorithm.  

ii. Finish Python and PyCuda algorithms. 

iii. Extra day to finish algorithms if necessary. 

iv. Construct benchmarking program and run benchmark.  

v. Construct gnuplot graphs of results. 

c. Week 3
---------
i. Determine parallel algorithm specification for matrix
multiplication. 

ii. Construct sequential implementations (super easy, get head start
on day 3). 

iii. Construct CUDA algorithm.

iv. Construct PyCUDA algorithm. 

v. Construct gnuplot graphs of results. (This might spill into week 4
because of problems in solving the shared memory access problem for
the various additions that take place between threads).  

d. Week 4
---------
i. Try python list copy instead of numpy.copy() to see if PyCUDA is then 
   significantly slower.
   Convert Mandelbrot and Matrix Multiplication graphs to n based on
   number of pixels and number of elements in Matrix.
   Run the PyCUDA and CUDA gpu increase-in-number-of-frames graph on a
   higher resolution (step size of 5). 
ii. Write sequential algorithms for CPU for the simulator. 
    Read Matrix Multiplication complexity paper. 
    Run Python sequential algorithms. 
    Google around about that weird 8 block anomaly. 
iii. Analyze different ratios between quadratic vs linear graphs in
     simulator vs Mandelbrot.
iv. Continue writing paper, theory section. Try to explain rationale
    behind gpu time analysis and structure of algorithms. 
v. Finish all the problems that are going to come up. e.g. Theory on
   the anomalous behavior in the GPU particle simulation graph. 


--->OLDPLAN {
i. Determine parallel algorithm specification for N Queens problem.

ii. Construct sequential implementations.  

iii. Construct CUDA algorithm.

iv. Construct PyCUDA algorithm. 

v. Construct gnuplot graphs of results.  (Again anticipating that this
week will spill into week 5 because a parallel implementation of the N
Queens problem is an interesting problem, and not too
straightforward). 
}

e. Week 5
---------
1. Look into how other research has developed parallel compuation models
   for performance. Continue working on model. 

2. Depending on how the model comes along, either continue
   researching other models that have been developed or test accuracy
   with the data that has been collected. 

3. Continue testing with the model. How can we tell if a model
   accurately describes the behavior of the GPU?

4. Is the model generalizable to any serial algorithm?
   The model is designed to take a serial algorithm performance
   equation and convert it to the performance of the corresponding
   naive parallel algorithm. What are the conditions behind the given 
   serial algorithm in terms of its parallelizability (a.k.a the
   process of parallelizification, or the quality of
   parallelalispicacity). 

5. Is there an exact threshold at which a specific type of serial
   algorithm, by using the model, will produce the exact same
   performance on the CPU as on the GPU? Is it possible to come up
   with a mathematical problem that has that property? (i.e. its
   problemathematicalizificationability? An important question.)

--->OLDPAN {
i. Finish up any spill over from above. 
}


2. Create model for GPU performance in terms of global memory
   accesses. Finish papers. 
=================================================================
OLD{
2. Study parallel algorithm complexity and write paper (3+ weeks)
-----------------------------------------------------------------
}

Week 6
------
The progress this week again depends on the unknown factor of whether
the model can be made more accurate. 

1-3. Isolate the reason why the predictions are very far off for the
     simulator, but not too far off for the mandelbrot set. 
     Fix the problem when/if found. 
     How accurate does the model need to be in order to say that it
     works effectively? What's the target accuracy?

4-5. Does the model work under nonconstant workloads on the CPU? The
     entire time_cpu(n) term is currently being divided by the number
     of parallel threads. However, it is not necessarily the case that
     everything that the CPU is doing is being parallelized, in fact,
     it is basically never the case. Some of it is not parallelized,
     so how do we fix this problem with the model? Of course, we need
     to prioritize fixing the current bigger problems with the
     model. If this were the current problem, then the GPU curve would
     theoretically be just below the CPU line, because it is
     predicting that everything is parallelized. This is not currently
     the case, so we don't need to focus on it immediately. 

--->OLDPLAN {
i. Ask critical questions about the complexity class of the algorithms
that were run. Are PyCuda and CUDA about the same runtime, or do they
differ in a manner that is statistically significant? If the latter,
then is it a difference by a constant ratio as N increases? Or does
their runtime appear to differ by more than a linear increase? (I
would think the former to be true, because the complexity should be
the same if the algorithms were written the same way).

ii. Can we determine ratios between the various curves that were
generated from gnuplot? Can we determine complexity class from the
curves? If so, what is the difference in complexity between the
parallel and sequential implementations?  
}

Week 7
------
i. Based on the data, what about the benchmark programs analyzed makes
them more or less suited to GPU computation? (i.e. Mandelbrot versus N
queens). 

ii. Can the independence of algorithms be classified? Algorithms can
be given a Big-O complexity for run-time, so could a similar form of
mathematics be applied to computational independence between the steps
of any algorithm?

Week 8
------
i. Write research paper. (And the other paper?)

Note: Whenever writing a benchmark algorithm for the 4 different
program types, the order should be either [C++, CUDA, Python, PyCuda]
or [C++, Python, CUDA, PyCUDA].  The first groups by language, while
the second groups by sequential versus parallel implementation.
