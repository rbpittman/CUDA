Meeting:
---------

I forgot to __syncthreads() before doing the write. Now the simulation doesn't 
change randomly. 
The weird data anomaly occurs when the number of blocks go past a multiple of 8. 
When graphing the Mandelbrot set benchmarks it abbreviates the x axis to 1e7 factors. 
  Is that ok? The other graph does a wierd thing with the zeros. And the fastest
  3 graph on Python shoots off pretty fast. 

TODO:
Analyze what can be said about ratios between python and PyCUDA. 
Write theory section. 
Find more modern paper convention on the graph unit label. 


Meeting:
--------

Wrote a Python program to compute the best fit line. So the ratio
between the slow matrix multiplication and fast is 5.68. 
Ran Python 12 hour bench on matrix multiplication. That's why I
couldn't draw out those ratios yet. 
Found paper that talks about step vs work complexity. 
idea: To keep it simple, what if I analyze performance gain from a
naive implementation of various algorithms, perhaps a classification
based on an estimation of the expected number of read/write accesses
on global memory. 

PLAN:
Find out more about step complexity vs work complexity. 
Planning on writing a program that prints out the difference between
python and pycuda, and c++ and cuda. 

TODO:
Difference between the model for gpu execution paper and the "idea"
mentioned above. 
Backup researchMachine. 


Meeting:
--------
Found another good paper on parallel complexity and that step
complexity vs work complexity. 
REALLY weird stuff going on with the best-power-finder. 
C++Long curve had 1.626 power. It's 1.553 when cut to normal times. 

Diff from the other paper: They are trying more to develop a model
 that allows one to write code to maximize performance by using various
 memory types in the GPU. Mine is developing a way by which a serial
 algorithm's performance boost from the conversion to naive GPU
 parallel algorithm can be modeled simply from the structure of the
 serial algorithm. 

How to write a theory section without knowing what the main objective
is?

Am I doing the 8-queens problem next week?

PLAN:

Draw the graph for the simulator, where the gpu performance is slower
than CPU. 
Do something along the lines of the classification of the number of
global read/writes required to complete a problem as a function of the
input size. 

TODO:
Rewrite schedule for next week.
Calculate total C++Long runtime
google about the operating system setting the priority of the job. 
Maybe set the priority to high. 


Meeting:
--------



Meeting:
--------
Running benchmark, will see in the morning whether the graph is better. 


--->
The power analysis on the "mean" python turned up the same power. 
--->

--->
Got some weird behavior with the GPU. 
--->


Plan:
-----
Try to find some paper that talks about mathematical models for
parallel computation. Find some equations. 


TODO:
-----

Use an ax^2 + bx + c. bx is the matrix multiplication generation. 
Keep working on testing global memory access. 
Find papers on modeling gpu performance execution. 


Meeting:
--------

Trying to determine the operations relating to global memory that
have n as a factor in the time equation for that operation. 
Figured out how to test the global diff performance. 
Crazy nvcc compiler optimizations:
This runs in O(steps) time:
for(int i = 0; i < steps; i++) {
  c += i;
}

This runs in constant time, still producing the correct result:
for(int i = 0; i < steps; i++) {
  c++;
}

Tried to figure out the equation for the 1.73 curve. No such luck. 


TODO:
-----
Figure out the equation for the memcpy function based on size. 
Using max memcpy size performed, figure out greatest time effect. 



Meeting:
--------
Equation for GPU 1.79931515731e-05 * x^1.0 + 0.142268967261

malloc testing did NOT go well at all! It does weird suballocation 
things that affect the run-time performance. 
Actually, methinks i got it for the worst case. 
Time to allocate and free 10000 times for various sizes has now been measured. 
If I don't free, it keeps using more and more memory. 
It does something weird where if I send in a value that is far to big, 


TODO:
-----
Test cudaMemset and get its complexity. 
Recreate model in more detail including all the types of global memory access. 

PLAN:
-----
Brief look at what particles simulations do for initial velocity. 


Meeting:
--------
memset is also linear. 

TOMORROW MORNING:
-----------------
Get rid of "First el" print statement. 
Then bench it with a script. 
Then do the same thing with the runRegisters function. 

THEN do the same thing with writing. 


PLAN:
-----
Brief look at what particles simulations do for initial velocity. 
Accurately test the access time difference between global memory read
and write. 


Meeting:
========

Made cool modeling program that shows the projected GPU performance. 

Used model prediction on the Mandelbrot set. Off by 2.5 coefficient. 

Model is pretty far off for simulator. 

Only just started debugging model, so it could be easy or very
difficult. 

Realized that this model has nothing to do with Python or PyCUDA. The
PyCUDA host code is Python, but GPU code is C++. 

Discuss plan ideas. problem with the model's theoretical accuracy. 


PLAN:
-----
Attempt to fix model. 


Other:
------

laptop might burn out. 

hiking.


Tomorrow:
---------
Figure out how read and writes with the GPU are handled. Why is it so
much faster than my model predicts?


Meeting:
========

Found cool command nvclock to get clock speed. 

Idea: What if the problem is that I'm not just iterating a single
thread in the actual benchmarks, I'm iterating multiple threads?

Do some tests to see if the GPU can do some form of parallel access:

Test 1:
Instead of testing iterating a single thread over the elements of an 
array, try running a situation where each thread accesses the 
i = ((blockDim.x * blockIdx.x) + threadIdx.x) element of an
array. Then we can see if the time taken is faster than one thread. 
Maybe make all the threads compute the sum of the array. 

Global access test:
We iterate over the array with all the threads, adding to local sum
the ith element of the array. Then we place the final sum in that
array it the threads index location. 
Register access test:
Same, except instead of adding the ith element, we add the value of
i.

Test 2:
Keep the number of global accesses the same, except this time run an
increasing number of threads all doing the same thing. See if the
execution time increases according to the same equation (this new
equation will be in terms of the number of global memory accesses, not
the number of threads). 

Plan:
-----

Write up the first test to see if it differs greatly from the initial
global mem read test. 

Global write:
1. Write a program that maximizes global memory efficiency. 
   Write the equivalent in register format. 
2. Write a program that minimizes global memory efficiency. (0, 16)
   Write the equivalent in register format. 


Tomorrow:
---------
Write a benchmark script for the
~/CUDA/read_write_tests/writeGlobal.cu program. 



Meeting:
--------
Found the sawtooth ridge data. This happens for all 4 data samples. 
Got a much more reasonable estimate for the write and read times:
7.287326297999999e-10 is the new parallel write. 
1.0413714335299995e-09 is the new parallel read. 

MUCH BETTER SUCCESS WITH THE NEW VALUES! tada what what what what what what what what 


Tomorrow:
---------
Plug in the new read and write times to the model for the simulator
to see if it is any better. 
Try to figure out why sawtooth ridge data exists. 
