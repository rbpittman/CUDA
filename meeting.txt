Meeting:
---------

I forgot to __syncthreads() before doing the write. Now the simulation doesn't 
change randomly. 
The weird data anomaly occurs when the number of blocks go past a multiple of 8. 
When graphing the Mandelbrot set benchmarks it abbreviates the x axis to 1e7 factors. 
  Is that ok? The other graph does a wierd thing with the zeros. And the fastest
  3 graph on Python shoots off pretty fast. 

TODO:
Analyze what can be said about ratios between python and PyCUDA. 
Write theory section. 
Find more modern paper convention on the graph unit label. 


Meeting:
--------

Wrote a Python program to compute the best fit line. So the ratio
between the slow matrix multiplication and fast is 5.68. 
Ran Python 12 hour bench on matrix multiplication. That's why I
couldn't draw out those ratios yet. 
Found paper that talks about step vs work complexity. 
idea: To keep it simple, what if I analyze performance gain from a
naive implementation of various algorithms, perhaps a classification
based on an estimation of the expected number of read/write accesses
on global memory. 

PLAN:
Find out more about step complexity vs work complexity. 
Planning on writing a program that prints out the difference between
python and pycuda, and c++ and cuda. 

TODO:
Difference between the model for gpu execution paper and the "idea"
mentioned above. 
Backup researchMachine. 


Meeting:
--------
Found another good paper on parallel complexity and that step
complexity vs work complexity. 
REALLY weird stuff going on with the best-power-finder. 
C++Long curve had 1.626 power. It's 1.553 when cut to normal times. 

Diff from the other paper: They are trying more to develop a model
 that allows one to write code to maximize performance by using various
 memory types in the GPU. Mine is developing a way by which a serial
 algorithm's performance boost from the conversion to naive GPU
 parallel algorithm can be modeled simply from the structure of the
 serial algorithm. 

How to write a theory section without knowing what the main objective
is?

Am I doing the 8-queens problem next week?

PLAN:

Draw the graph for the simulator, where the gpu performance is slower
than CPU. 
Do something along the lines of the classification of the number of
global read/writes required to complete a problem as a function of the
input size. 

TODO:
Rewrite schedule for next week.
Calculate total C++Long runtime
google about the operating system setting the priority of the job. 
Maybe set the priority to high. 


Meeting:
--------



Meeting:
--------
Running benchmark, will see in the morning whether the graph is better. 


--->
The power analysis on the "mean" python turned up the same power. 
--->

--->
Got some weird behavior with the GPU. 
--->


Plan:
-----
Try to find some paper that talks about mathematical models for
parallel computation. Find some equations. 


TODO:
-----

Use an ax^2 + bx + c. bx is the matrix multiplication generation. 
Keep working on testing global memory access. 
Find papers on modeling gpu performance execution. 


Meeting:
--------

Trying to determine the operations relating to global memory that
have n as a factor in the time equation for that operation. 
Figured out how to test the global diff performance. 
Crazy nvcc compiler optimizations:
This runs in O(steps) time:
for(int i = 0; i < steps; i++) {
  c += i;
}

This runs in constant time, still producing the correct result:
for(int i = 0; i < steps; i++) {
  c++;
}

Tried to figure out the equation for the 1.73 curve. No such luck. 


TODO:
-----
Figure out the equation for the memcpy function based on size. 
Using max memcpy size performed, figure out greatest time effect. 



Meeting:
--------
Equation for GPU 1.79931515731e-05 * x^1.0 + 0.142268967261

malloc testing did NOT go well at all! It does weird suballocation 
things that affect the run-time performance. 
Actually, methinks i got it for the worst case. 
Time to allocate and free 10000 times for various sizes has now been measured. 
If I don't free, it keeps using more and more memory. 
It does something weird where if I send in a value that is far to big, 


TODO:
-----
Test cudaMemset and get its complexity. 
Recreate model in more detail including all the types of global memory access. 

PLAN:
-----
Brief look at what particles simulations do for initial velocity. 


Meeting:
--------
memset is also linear. 

TOMORROW MORNING:
-----------------
Get rid of "First el" print statement. 
Then bench it with a script. 
Then do the same thing with the runRegisters function. 

THEN do the same thing with writing. 


PLAN:
-----
Brief look at what particles simulations do for initial velocity. 
Accurately test the access time difference between global memory read
and write. 


Meeting:
========

Made cool modeling program that shows the projected GPU performance. 

Used model prediction on the Mandelbrot set. Off by 2.5 coefficient. 

Model is pretty far off for simulator. 

Only just started debugging model, so it could be easy or very
difficult. 

Realized that this model has nothing to do with Python or PyCUDA. The
PyCUDA host code is Python, but GPU code is C++. 

Discuss plan ideas. problem with the model's theoretical accuracy. 


PLAN:
-----
Attempt to fix model. 


Other:
------

laptop might burn out. 

hiking.


Tomorrow:
---------
Figure out how read and writes with the GPU are handled. Why is it so
much faster than my model predicts?


Meeting:
========

Found cool command nvclock to get clock speed. 

Idea: What if the problem is that I'm not just iterating a single
thread in the actual benchmarks, I'm iterating multiple threads?

Do some tests to see if the GPU can do some form of parallel access:

Test 1:
Instead of testing iterating a single thread over the elements of an 
array, try running a situation where each thread accesses the 
i = ((blockDim.x * blockIdx.x) + threadIdx.x) element of an
array. Then we can see if the time taken is faster than one thread. 
Maybe make all the threads compute the sum of the array. 

Global access test:
We iterate over the array with all the threads, adding to local sum
the ith element of the array. Then we place the final sum in that
array it the threads index location. 
Register access test:
Same, except instead of adding the ith element, we add the value of
i.

Test 2:
Keep the number of global accesses the same, except this time run an
increasing number of threads all doing the same thing. See if the
execution time increases according to the same equation (this new
equation will be in terms of the number of global memory accesses, not
the number of threads). 

Plan:
-----

Write up the first test to see if it differs greatly from the initial
global mem read test. 

Global write:
1. Write a program that maximizes global memory efficiency. 
   Write the equivalent in register format. 
2. Write a program that minimizes global memory efficiency. (0, 16)
   Write the equivalent in register format. 


Tomorrow:
---------
Write a benchmark script for the
~/CUDA/read_write_tests/writeGlobal.cu program. 



Meeting:
--------
Found the sawtooth ridge data. This happens for all 4 data samples. 
Got a much more reasonable estimate for the write and read times:
7.287326297999999e-10 is the new parallel write. 
1.0413714335299995e-09 is the new parallel read. 

MUCH BETTER SUCCESS WITH THE NEW VALUES! tada what what what what
Simulator is a little low, but when I scale it to fit, it's a 1.81 
power curve, not 2.00. I think it's because the x**2 term is too 
small, because the read and writes are very small. So maybe in the
best case that is actually what should happen. The power should appear 
to be smaller. The only way to test this is too actually test the worst 
read/write case possible. 

NOTE:
-----
http://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/
has good information on global memory accesses. My device is compute capability
1.1, so it only allows for global memory coalescing for a half warp, where 
a warp is 32 threads. 


Meeting:
--------
Read up and found a good page as listed in the note above^.



Tomorrow:
---------
Write the parse file for the benchmark that was running. 
Then download the individual files and compute the equation for each
to see which is the slowest. 
Wrong {
2 should be fastest,
4 should be a bit slower,
8 should be even slower,
16 should be the slowest because it jumps over the half warps,
32 should be the same as 16.
} wrong because none of the above step settings ever uses any form
  of global memory coalescing, so they should all be about the 
  same speed. 

TODO:
try the step size for 1. 


lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
NO DR. B
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll

======
Monday
======

Write a test that computes the curve f(n) where n is the number of
threads simultaneously being executed. Test up to 4 or so full blocks
of threads. 
Prints out the number of currently active warps. 
Checking to see if scaling by the same constant produces viable
results. 

Model problem: Currently the gap is too low, trying to find the term
that needs to be increased.

Can't scale the whole model, because the simulator would then be 
wrong: the curve goes above, then below. Not good. 

Can't simply divide the number of processes simultaneously executing,
because then the simulator inefficient curve is still too low, while
the Mandelbrot set curve is dead on. 


Ran tests: computed primes in each thread and timed how long it took
for an increasing number of threads. 
Computed primes in each threadblock (32 threads per block), and timed
how long it took for an increasing number of blocks. 
Reading information that might explain some of the strange behavior of
the GPU. Need to REALLY learn how it works.

Found out something interesting! For some reason, when I reboot, a
CUDA program will actually run if I run it in sudo mode. From then on,
any program will work for the GPU. The same goes for nvidia-smi. 

"Fixed" the problem with the reboot not activating the GPU. Put
nvidia-smi command into the file /etc/rc.local to make it "sudo"
activate the GPU. 

Tomorrow:
---------
Continue learning how the GPU REALLY works. 


=======
Tuesday
=======

Got my ubuntu back in 3D mode. lol. sudo apt-get purge nvidia*

Maybe instead of modeling based on the number of concurrent. 

INSANE: Finally realized that I do not need to worry about the limits
of shared mem on the simulator because I am already limiting the
number of threads to less than 512! So I wrote a quick shared mem
simulator, benched it, and it is currently below the best case bounds
for my model!! 

How do I determine the GPU's "quality of parallel execution"?
i.e., exactly how many things does it do simultaneously?

Need to continue looking into the quality of parallelism of the GPU,
and exactly how the GPU does things simultaneously. 

Finding that, found that it looked linear, and then
quadratic. Realized that that was because initially all the cores were
not being used, so it should have been much more flat. 

Task: Run a new benchmark to determine parallelism more accurately by 
running it up to 1024. 

===========           
 Wednesday           (...)                      ~
===========         (......)        \ /          
                      (..)        ---O---          ~
           ^                        / \              ~
          / \                                       ~
          |~|                                   ~ 
          |U|                                         
          |S|
     *   /|A|\ 
 *      /_|_|_\  *  *           *
     *    /^\     *      *
   * ** *** **** ** * 
* *  ______________ *        *
 * _/--------------\_   *
 _/------------------\_
 ^^^^^^^^^^^^^^^^^^^^^^

Graphics card is listed here, I think:
http://www.gigabyte.com/products/product-page.aspx?pid=4097#ov
Model name:
GV-N84S-512I (rev. 1.4) 
Learning more about what GPU cores are. 

Yesterday worked on finding the actual number of simultaneous
threads. About 144 was the result. 
Today need to figure out what it should be, then see if that 
result is any better. 
Determined (hopefully accurately) that the number of cores on 
my device is 8. 
This is based on this URL:
http://www.gpuzoo.com/Compare/NVIDIA_GeForce_8400_GS_Rev_2__vs__GIGABYTE_GV-N84S-512I_%28rev._1.4%29/
I have the one on the right because it has a PCI-Express 2.0, not 1.0 
Assuming that each core processes one warp (32 threads), then there are 
8 * 32 = 256 lines of simultaneous execution. 

Still have a big problem: Simulator worst case is still too low. 
Works nicely for Matrix Mult.
Still decently off for Mandelbrot. 
QUESTION: Why is the simulator in real programming so much slower
than my model predicts it to be?
Does it maybe have to do with the fact that some stuff is only done on
the cpu? and thus not in parallel?
Maybe it is because the parallelism level is dropping because of split
data: The threads are going different directions and slowing down?
Determine whether that is the case....

Interesting and easily testable question: 
What happens to the tested value of parallelality if every other
thread executes a different instruction. 

REALIZATION: My current method of testing parallelality has a freaking
IF statement! That means that the scheduler (sheduler) has had to
break up the program into separate warps of execution! I think! 

ok, just compute an iSum!
Retesting....
ANOTHER REALIZATION: while it does have an if statement, all threads
follow the if statement the same way. still retesting...

Tomorrow:
---------

When that is done testing, copy it over and graph it, and run the 
same parallel analysis program on it as the old one. 
Hopefully it gives a significantly different value. If not, we are 
confirming that the original value we got is pretty good. 
Current theory as to why the actual simulator is so slow: There is
some form of thread if stmt breaking that is causing warp divergence. 
Check this theory tomorrow. 

==========
Thursday
==========

Realization: In order to test the absolute worst case for the GPU,
shouldn't there be maximum warp divergence? When writing a naive
implementation, this would theoretically be possible. In the best
case, as tested with the iSum, the parallelality value is 165. 
Maybe now we should test a maximum warp divergence. 

Now check if there is warp divergence in the simulator code. 
Code structure:
__device__ inline void computeForce(float * positions, float * vels,
				    bool * inBounds, int idx,
				    int numParts) {
  for(int i = 0; i < numParts; i++) {
    if((i != idx) && (inBounds[i])) {
      if(denom != 0) {
      }
    }
  }  
}

__global__ void runframe(float * positions, float * vels,
			 bool * inBounds, int numParts) {
  if(idx < numParts) {
    if(inBounds[idx]) {
      computeForce(positions, vels, inBounds, idx, numParts);
    }
  }
}

It appears that there is in fact some serious warp divergence. Here's
why: The first warp will execute the first 32 threads. However, the
first thread has an if that makes sure that i != 0. But it does, so it
skips that execution, so it waits while the others go in the loop,
etc. Serious problem with respect to warp divergence! This would
explain why the simulator is so slow with respect to max bounds!

Next task--> Write code to test parallel execution with maximum thread
divergence. See if this value is significantly small enough, like 60. 




REALIZATION: The Mandelbrot set program has SERIOUS warp divergence!
for(i = 0; i < max_depth; i++) {
  if(ABS(real, imag) > EXCEED_VALUE)
    break;
I honestly don't have any good ideas on how to fix this. The
efficiency problem here is that the program could potentially take
advantage of the fact that certain pixels are easier to compute, and
thus finish some computation early. The problem is that with the
current if stmt and warp divergence, threads that finish early just
sit around and wait, taking up SM warp space that could be spent on
other pixels. That would be a difficult problem to solve. 

This does affect the model because the CPU does not have to complete
that loop. The model assumes that everything that the CPU does is
parallelized by the number of threads running (or parallelality). In
the current Mandelbrot, not everything is parallelized because some
threads are sitting around waiting. It's at the border between the
white and black sections that this is a problem. 

Tomorrow:
---------
Look at the end of
~/CUDA/test_parallel_speed/PROC.txt
and see if the new parallelality value is good or not, and test the
new model graphs. 

======
Friday
======

5 is the new value. This makes sense because of the huge inefficiency
in the code, it should barely be able to do anything in parallel. 

Matrix multiplication lower bounds is close to actual, indicating that
my naive algorithm is very close to optimal when using global memory. 

Mandelbrot set is still just below the lower bounds. The parallelality
value would need to be 190. The major component of the Mandelbrot time
in the model is the cpu to gpu conversion time. 

Just to double check: Let's test the parallelality finder on single
thread for the CPU, to see if the value for the CPU to GPU ratio is
the same. 
Found a very similar value of 30.5 or so. That's good. 

TODO: Maybe figure out how to make the Mandelbrot set more accurate. 
Current factor to make it on target is: 0.86
TODO: Write paper. 

Writing the introduction to the paper. 

======
Monday
======

The Mandelbrot set algorithm has that delay warp divergence, so it's
doing some "computation" that it shouldn't have to based on the CPU
had to do. This means that the CUDA data should definitely be above
the lower bounds. 

done TODO: Revise plan. 
done TODO: Read book. 
TODO: Figure out which term is off in the cpu_to_gpu function for the
Mandelbrot set lower bounds. 
TODO: Answer what to do with some extra time. 
TODO: Answer paper questions. Is there a good handbook?


Realized: Maybe I can just do the parallelize test on the Mandelbrot
          set kernel.

Testing... Nope, not simple to do, or maybe just not worth it. 

IDEA, before pursuing the above test: Maybe the reason why the
apparent Mandelbrot parallelality is so high is that it has tons of
threads executing simulataneously, and so maybe Matrix Multiplication
is a bit off too. 

Test larger number of threads. Maybe it's the write at the end that is
slowing down the parallelality, because all the threads might be
waiting to do the write. 

Tomorrow
--------
That benchmark at 
~/CUDA/test_parallel_speed/benchParallelISumLarge.txt
should be done. Run it through the Parallel equation guess py file,
and find new parallelality value. 

IDEA: Maybe it's a problem with the power curve on the CPU Mandelbrot
set. Retest the powers on the CUDA and C++ curves. 
Test: Maybe the reason the power on the Mandelbrot is 0.93 is because
of the higher parallelality on larger numbers of threads.

Tested super large number of threads, only got 167. Still not good. 
Maybe find out where CUDA kernel parameters are stored in device mem. 
Maybe I could test it on a much larger image space and see a more
detailed picture of the curve. 

Max number of pixels is 2**(16) * 512

REALIZATION: Kind of figured out why the power on the Mandelbrot 
set computeBestCurve.py program is 0.93: It's because the larger x
values are further apart, and thus subject to more error towards the
right side of the graph. 

Result: Tested larger number of threads for benchmarking the
Mandelbrot set: New equation power is 0.98: Much better, I wonder what
the model predicts for the best case for that...
drum roll please...
dadadadadadadadadadadadadadadadadadadadadadadada.................
woops, forgot that the depth is 500 for the current cpu test, need 
to increase the depth for the CPU and do a long run. 

Wait till C++ large is done running for depth of 1000 so we can see
whether the new larger CUDA data set is better. 
In the meantime...
Found a good paper from 1989 that describes the PRAM model. 
Done da bench.
repeat drumroll please...
dadadadadadadadadadadadadadadadadadadadadadadada.................
Dangit, exactly the same amount above the actual data. 
But wait, is it the exact same amount???
Dude, it's 0.83 for the large data set, and 0.86 for the initial
smaller data set. That's pretty close. 

Tomorrow
--------
Let's look over the code and see if something in there would 
explain why the CPU is going slower than it should be. 
Maybe figure out how to explain the problem to scotty or dr Shende. 


Meeting
=======
2 possibilities at this point:
 1. The CPU is doing something that at 7.95 seconds execution time is causing it
    to take an extra 1.13 seconds to do than it should. 
 2. The GPU and CPU thread ratio is slower than it 

TEST: Test to see what value the gpu-cpu thread ratio would have to be. 
Guess what... You multiply 0.86 by 30 to get about 26 for the required
 gpu-cpu ratio, dat makes sense, but it's annoying that that value keeps
 coming up. 

Maybe I need to take the clock speed ratios...


Talked to professor scott:
Check to see if you can turn off nvcc optimizzzzzation. 
Can try moving the break. 

TESTING: Running a test of gpuMand.cu with all optimization flags off:
         nvcc -O0 -Xopencc -O0 -Xptxas -O0 -c gpuMand.cu
    Going to see if this new curve is above the worst case bounds. 
    If this works, I might need to retest all of the simulator and 
    matrix multiplication CUDA code. 

Hmmm pretty meh results.

Tomorrow
--------
Analyze the meh results. 
The problem with that theory is that I am basically testing the GPU
when it has those optimizations in the for loop. 
New idea: test parallelality with optimizations off. 

WELL, THERE'S ONLY ONE THING LEFT TO DO.....
TEST: Parallelality value for the actual Mandelbrot code, and 
the gpu-cpu thread speed ratio for the Mandelbrot code. 

SWEET: Found a new best case for thread ratio: 18.5. 
This is why the Mandelbrot set is so much faster than my model predicts. 
19.6 is for no optimizations. 
Maybe we can optimize the CPU to get a better time for the gpu-cpu ratio.

Turn on max cpu optimization with loop unrolling. 
Testing new thread speed execution ratio. 

Tomorrow
--------
Find out the slopes of the lines generated from the files 
~/CUDA/cpu_to_gpu_ratio/timeGPUThreadWithCPUOpts.txt
and
~/CUDA/cpu_to_gpu_ratio/timeCPUThreadWithCPUOpts.txt
Then get the ratio between them and see if it's closer to 30. 
