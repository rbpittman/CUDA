 Meeting:
---------

I forgot to __syncthreads() before doing the write. Now the simulation doesn't 
change randomly. 
The weird data anomaly occurs when the number of blocks go past a multiple of 8. 
When graphing the Mandelbrot set benchmarks it abbreviates the x axis to 1e7 factors. 
  Is that ok? The other graph does a wierd thing with the zeros. And the fastest
  3 graph on Python shoots off pretty fast. 

TODO:
Analyze what can be said about ratios between python and PyCUDA. 
Write theory section. 
Find more modern paper convention on the graph unit label. 


Meeting:
--------

Wrote a Python program to compute the best fit line. So the ratio
between the slow matrix multiplication and fast is 5.68. 
Ran Python 12 hour bench on matrix multiplication. That's why I
couldn't draw out those ratios yet. 
Found paper that talks about step vs work complexity. 
idea: To keep it simple, what if I analyze performance gain from a
naive implementation of various algorithms, perhaps a classification
based on an estimation of the expected number of read/write accesses
on global memory. 

PLAN:
Find out more about step complexity vs work complexity. 
Planning on writing a program that prints out the difference between
python and pycuda, and c++ and cuda. 

TODO:
Difference between the model for gpu execution paper and the "idea"
mentioned above. 
Backup researchMachine. 


Meeting:
--------
Found another good paper on parallel complexity and that step
complexity vs work complexity. 
REALLY weird stuff going on with the best-power-finder. 
C++Long curve had 1.626 power. It's 1.553 when cut to normal times. 

Diff from the other paper: They are trying more to develop a model
 that allows one to write code to maximize performance by using various
 memory types in the GPU. Mine is developing a way by which a serial
 algorithm's performance boost from the conversion to naive GPU
 parallel algorithm can be modeled simply from the structure of the
 serial algorithm. 

How to write a theory section without knowing what the main objective
is?

Am I doing the 8-queens problem next week?

PLAN:

Draw the graph for the simulator, where the gpu performance is slower
than CPU. 
Do something along the lines of the classification of the number of
global read/writes required to complete a problem as a function of the
input size. 

TODO:
Rewrite schedule for next week.
Calculate total C++Long runtime
google about the operating system setting the priority of the job. 
Maybe set the priority to high. 


Meeting:
--------



Meeting:
--------
Running benchmark, will see in the morning whether the graph is better. 


--->
The power analysis on the "mean" python turned up the same power. 
--->

--->
Got some weird behavior with the GPU. 
--->


Plan:
-----
Try to find some paper that talks about mathematical models for
parallel computation. Find some equations. 


TODO:
-----

Use an ax^2 + bx + c. bx is the matrix multiplication generation. 
Keep working on testing global memory access. 
Find papers on modeling gpu performance execution. 


Meeting:
--------

Trying to determine the operations relating to global memory that
have n as a factor in the time equation for that operation. 
Figured out how to test the global diff performance. 
Crazy nvcc compiler optimizations:
This runs in O(steps) time:
for(int i = 0; i < steps; i++) {
  c += i;
}

This runs in constant time, still producing the correct result:
for(int i = 0; i < steps; i++) {
  c++;
}

Tried to figure out the equation for the 1.73 curve. No such luck. 


TODO:
-----
Figure out the equation for the memcpy function based on size. 
Using max memcpy size performed, figure out greatest time effect. 



Meeting:
--------
Equation for GPU 1.79931515731e-05 * x^1.0 + 0.142268967261

malloc testing did NOT go well at all! It does weird suballocation 
things that affect the run-time performance. 
Actually, methinks i got it for the worst case. 
Time to allocate and free 10000 times for various sizes has now been measured. 
If I don't free, it keeps using more and more memory. 
It does something weird where if I send in a value that is far to big, 


TODO:
-----
Test cudaMemset and get its complexity. 
Recreate model in more detail including all the types of global memory access. 

PLAN:
-----
Brief look at what particles simulations do for initial velocity. 


Meeting:
--------
memset is also linear. 

TOMORROW MORNING:
-----------------
Get rid of "First el" print statement. 
Then bench it with a script. 
Then do the same thing with the runRegisters function. 

THEN do the same thing with writing. 


PLAN:
-----
Brief look at what particles simulations do for initial velocity. 
Accurately test the access time difference between global memory read
and write. 


Meeting:
========

Made cool modeling program that shows the projected GPU performance. 

Used model prediction on the Mandelbrot set. Off by 2.5 coefficient. 

Model is pretty far off for simulator. 

Only just started debugging model, so it could be easy or very
difficult. 

Realized that this model has nothing to do with Python or PyCUDA. The
PyCUDA host code is Python, but GPU code is C++. 

Discuss plan ideas. problem with the model's theoretical accuracy. 


PLAN:
-----
Attempt to fix model. 


Other:
------

laptop might burn out. 

hiking.


Tomorrow:
---------
Figure out how read and writes with the GPU are handled. Why is it so
much faster than my model predicts?


Meeting:
========

Found cool command nvclock to get clock speed. 

Idea: What if the problem is that I'm not just iterating a single
thread in the actual benchmarks, I'm iterating multiple threads?

Do some tests to see if the GPU can do some form of parallel access:

Test 1:
Instead of testing iterating a single thread over the elements of an 
array, try running a situation where each thread accesses the 
i = ((blockDim.x * blockIdx.x) + threadIdx.x) element of an
array. Then we can see if the time taken is faster than one thread. 
Maybe make all the threads compute the sum of the array. 

Global access test:
We iterate over the array with all the threads, adding to local sum
the ith element of the array. Then we place the final sum in that
array it the threads index location. 
Register access test:
Same, except instead of adding the ith element, we add the value of
i.

Test 2:
Keep the number of global accesses the same, except this time run an
increasing number of threads all doing the same thing. See if the
execution time increases according to the same equation (this new
equation will be in terms of the number of global memory accesses, not
the number of threads). 

Plan:
-----

Write up the first test to see if it differs greatly from the initial
global mem read test. 

Global write:
1. Write a program that maximizes global memory efficiency. 
   Write the equivalent in register format. 
2. Write a program that minimizes global memory efficiency. (0, 16)
   Write the equivalent in register format. 


Tomorrow:
---------
Write a benchmark script for the
~/CUDA/read_write_tests/writeGlobal.cu program. 



Meeting:
--------
Found the sawtooth ridge data. This happens for all 4 data samples. 
Got a much more reasonable estimate for the write and read times:
7.287326297999999e-10 is the new parallel write. 
1.0413714335299995e-09 is the new parallel read. 

MUCH BETTER SUCCESS WITH THE NEW VALUES! tada what what what what
Simulator is a little low, but when I scale it to fit, it's a 1.81 
power curve, not 2.00. I think it's because the x**2 term is too 
small, because the read and writes are very small. So maybe in the
best case that is actually what should happen. The power should appear 
to be smaller. The only way to test this is too actually test the worst 
read/write case possible. 

NOTE:
-----
http://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/
has good information on global memory accesses. My device is compute capability
1.1, so it only allows for global memory coalescing for a half warp, where 
a warp is 32 threads. 


Meeting:
--------
Read up and found a good page as listed in the note above^.



Tomorrow:
---------
Write the parse file for the benchmark that was running. 
Then download the individual files and compute the equation for each
to see which is the slowest. 
Wrong {
2 should be fastest,
4 should be a bit slower,
8 should be even slower,
16 should be the slowest because it jumps over the half warps,
32 should be the same as 16.
} wrong because none of the above step settings ever uses any form
  of global memory coalescing, so they should all be about the 
  same speed. 

TODO:
try the step size for 1. 


lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
NO DR. B
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll

======
Monday
======

Write a test that computes the curve f(n) where n is the number of
threads simultaneously being executed. Test up to 4 or so full blocks
of threads. 
Prints out the number of currently active warps. 
Checking to see if scaling by the same constant produces viable
results. 

Model problem: Currently the gap is too low, trying to find the term
that needs to be increased.

Can't scale the whole model, because the simulator would then be 
wrong: the curve goes above, then below. Not good. 

Can't simply divide the number of processes simultaneously executing,
because then the simulator inefficient curve is still too low, while
the Mandelbrot set curve is dead on. 


Ran tests: computed primes in each thread and timed how long it took
for an increasing number of threads. 
Computed primes in each threadblock (32 threads per block), and timed
how long it took for an increasing number of blocks. 
Reading information that might explain some of the strange behavior of
the GPU. Need to REALLY learn how it works.

Found out something interesting! For some reason, when I reboot, a
CUDA program will actually run if I run it in sudo mode. From then on,
any program will work for the GPU. The same goes for nvidia-smi. 

"Fixed" the problem with the reboot not activating the GPU. Put
nvidia-smi command into the file /etc/rc.local to make it "sudo"
activate the GPU. 

Tomorrow:
---------
Continue learning how the GPU REALLY works. 


=======
Tuesday
=======

Got my ubuntu back in 3D mode. lol. sudo apt-get purge nvidia*

Maybe instead of modeling based on the number of concurrent. 

INSANE: Finally realized that I do not need to worry about the limits
of shared mem on the simulator because I am already limiting the
number of threads to less than 512! So I wrote a quick shared mem
simulator, benched it, and it is currently below the best case bounds
for my model!! 

How do I determine the GPU's "quality of parallel execution"?
i.e., exactly how many things does it do simultaneously?

Need to continue looking into the quality of parallelism of the GPU,
and exactly how the GPU does things simultaneously. 

Finding that, found that it looked linear, and then
quadratic. Realized that that was because initially all the cores were
not being used, so it should have been much more flat. 

Task: Run a new benchmark to determine parallelism more accurately by 
running it up to 1024. 

===========           
 Wednesday           (...)                      ~
===========         (......)        \ /          
                      (..)        ---O---          ~
           ^                        / \              ~
          / \                                       ~
          |~|                                   ~ 
          |U|                                         
          |S|
     *   /|A|\ 
 *      /_|_|_\  *  *           *
     *    /^\     *      *
   * ** *** **** ** * 
* *  ______________ *        *
 * _/--------------\_   *
 _/------------------\_
 ^^^^^^^^^^^^^^^^^^^^^^

Graphics card is listed here, I think:
http://www.gigabyte.com/products/product-page.aspx?pid=4097#ov
Model name:
GV-N84S-512I (rev. 1.4) 
Learning more about what GPU cores are. 

Yesterday worked on finding the actual number of simultaneous
threads. About 144 was the result. 
Today need to figure out what it should be, then see if that 
result is any better. 
Determined (hopefully accurately) that the number of cores on 
my device is 8. 
This is based on this URL:
http://www.gpuzoo.com/Compare/NVIDIA_GeForce_8400_GS_Rev_2__vs__GIGABYTE_GV-N84S-512I_%28rev._1.4%29/
I have the one on the right because it has a PCI-Express 2.0, not 1.0 
Assuming that each core processes one warp (32 threads), then there are 
8 * 32 = 256 lines of simultaneous execution. 

Still have a big problem: Simulator worst case is still too low. 
Works nicely for Matrix Mult.
Still decently off for Mandelbrot. 
QUESTION: Why is the simulator in real programming so much slower
than my model predicts it to be?
Does it maybe have to do with the fact that some stuff is only done on
the cpu? and thus not in parallel?
Maybe it is because the parallelism level is dropping because of split
data: The threads are going different directions and slowing down?
Determine whether that is the case....

Interesting and easily testable question: 
What happens to the tested value of parallelality if every other
thread executes a different instruction. 

REALIZATION: My current method of testing parallelality has a freaking
IF statement! That means that the scheduler (sheduler) has had to
break up the program into separate warps of execution! I think! 

ok, just compute an iSum!
Retesting....
ANOTHER REALIZATION: while it does have an if statement, all threads
follow the if statement the same way. still retesting...

Tomorrow:
---------

When that is done testing, copy it over and graph it, and run the 
same parallel analysis program on it as the old one. 
Hopefully it gives a significantly different value. If not, we are 
confirming that the original value we got is pretty good. 
Current theory as to why the actual simulator is so slow: There is
some form of thread if stmt breaking that is causing warp divergence. 
Check this theory tomorrow. 

==========
Thursday
==========

Realization: In order to test the absolute worst case for the GPU,
shouldn't there be maximum warp divergence? When writing a naive
implementation, this would theoretically be possible. In the best
case, as tested with the iSum, the parallelality value is 165. 
Maybe now we should test a maximum warp divergence. 

Now check if there is warp divergence in the simulator code. 
Code structure:
__device__ inline void computeForce(float * positions, float * vels,
				    bool * inBounds, int idx,
				    int numParts) {
  for(int i = 0; i < numParts; i++) {
    if((i != idx) && (inBounds[i])) {
      if(denom != 0) {
      }
    }
  }  
}

__global__ void runframe(float * positions, float * vels,
			 bool * inBounds, int numParts) {
  if(idx < numParts) {
    if(inBounds[idx]) {
      computeForce(positions, vels, inBounds, idx, numParts);
    }
  }
}

It appears that there is in fact some serious warp divergence. Here's
why: The first warp will execute the first 32 threads. However, the
first thread has an if that makes sure that i != 0. But it does, so it
skips that execution, so it waits while the others go in the loop,
etc. Serious problem with respect to warp divergence! This would
explain why the simulator is so slow with respect to max bounds!

Next task--> Write code to test parallel execution with maximum thread
divergence. See if this value is significantly small enough, like 60. 




REALIZATION: The Mandelbrot set program has SERIOUS warp divergence!
for(i = 0; i < max_depth; i++) {
  if(ABS(real, imag) > EXCEED_VALUE)
    break;
I honestly don't have any good ideas on how to fix this. The
efficiency problem here is that the program could potentially take
advantage of the fact that certain pixels are easier to compute, and
thus finish some computation early. The problem is that with the
current if stmt and warp divergence, threads that finish early just
sit around and wait, taking up SM warp space that could be spent on
other pixels. That would be a difficult problem to solve. 
